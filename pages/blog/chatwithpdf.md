---
title: How to use ChatGPT with your own PDF documents
date: 2023-04-04
description: Analyze your PDF faster than reading the whole of it.
tag: AI, ChatGPT, Langchain, Pinecone
author: Unies Ananda Raja
---

![alt text for screen readers](/images/comingofagi.png "The coming of AGI") 

ChatGPT is pretty good, but it is quite limited. It is limited in its token length, or how many words the prompt and answer might be. The limit is 4096 tokens, which basically translates into about 8,000 words, or roughly four to five pages of document. It also has knowledge cutoff of 2021 so it won't know up-to-date information.

You might also want to use ChatGPT to talk about your specific documents, maybe work documentation, school handbook, non-fiction book, and many other. But your documents might be way longer than 8,000 words. That's a problem.

Also, ChatGPT might not have a clue what you're talking about and start making things up. This is called the hallucination problem. 

Now, you can actually avoid hallucination problem and provide ChatGPT with your own documents that are longer than the token limits. How? Embeddings.

**Introducing Embeddings**

Embeddings are vector representations of text strings that measure their relatedness. An embedding is a vector (list) of floating-point numbers, and the distance between two vectors measures their relatedness. Embeddings make it easier to perform machine learning on large inputs, exactly like what we want to do.

For this purpose, we are going to use OpenAI's embedding model called "text-embedding-ada-002".

We also need another two things: Langchain and Pinecone. 

**Introducing Langchain and Pinecone**

**Langchain** is a library designed to help developers build powerful applications by combining large language models (LLMs) with other sources of computation or knowledge. Langchain can be used for applications like question-answering over specific documents, exactly want we want to do. In this case, Langchain will act as the glue between the embedding model and the vector database.

**Pinecone** is a fully managed, cloud-native vector database that enables developers to build high-performance vector search applications. Our vectors generated by "text-embedding-ada-002" will be stored in a Pinecone index.


Let's go through this [Colab Notebook](https://colab.research.google.com/drive/1SB-eS_Wr-wt-XBZoMdixShuMomJURkSM#scrollTo=zXmHs7DpNa0G) and see how it works:


### Install the required libraries

```
!sudo apt install tesseract-ocr
!apt-get install poppler-utils
!pip install --upgrade pillow
!pip install langchain openai unstructured[local-inference] pinecone-client
!pip install "detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2"
!pip install layoutparser[layoutmodels,tesseract]
```

After installation, you should restart the runtime of the Colab notebook by clicking on the 'Runtime' tab at the top, and then selecting Restart runtime. After restarting, run your code again. Why? I don't exactly know.

### Upload your PDF

```
import os
from google.colab import files

# Set the directory path to "/pdf" folder
pdf_dir = '/content/pdf/'

# Create the folder if it doesn't exist
if not os.path.exists(pdf_dir):
    os.makedirs(pdf_dir)

# Upload the file and save it to the "/pdf" folder
uploaded_file = list(files.upload().keys())[0]
os.rename(uploaded_file, pdf_dir + uploaded_file)
```
You can upload one or multiple files.

### Load your PDF

```
from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader('/content', glob="**/*.pdf")
docs = loader.load()
```
Here we use Langchain to load the PDF. The loading process might take a couple of minutes or a while, depends on the PDF size.

### Split your PDF into smaller chunks

```
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(docs)
```
Here we also use Langchain to split the PDF into chunks.

### Input your keys
```
OPENAI_API_KEY = 'OPENAI_API_KEY' #Replace with your own OpenAI API key
PINECONE_API_KEY = 'PINECONE_API_KEY' #Replace with your own Pinecone key
PINECONE_API_ENV = 'PINECONE_API_ENV' #Replace with your own Pinecone environment
```
Get your OpenAI API key by going to https://platform.openai.com/account/api-keys and make sure you already have an account.


For Pinecone, you can sign up for an account. Then, go to "API Keys"
![alt text for screen readers](/images/pinecone1.png "API Keys") 

### Create vectors/embedding from your PDF

```
from langchain.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone

embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
```

### Store vectors/embedding from your PDF on Pinecone

```
from langchain.vectorstores import Chroma, Pinecone
import pinecone

index_name = 'test' # change into your own index

# initialize connection to pinecone
pinecone.init(
    api_key=PINECONE_API_KEY,  # app.pinecone.io (console)
    environment=PINECONE_API_ENV  # next to API key in console
)

index = pinecone.Index(index_name)

docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)
```

Make sure you have created your Pinecone index. After signing up, create your first index. The name is up to you. The dimensions should be 1536, as it is the dimension of vectors from OpenAI. Then, choose "P1". 
![alt text for screen readers](/images/pinecone2.png "Index") 

### Query and retrieve vectors/embedding from Pinecone

```
import openai
import pinecone
embed_model = "text-embedding-ada-002"


query = "What is Langchain?" #Change your question

res = openai.Embedding.create(
    input=[query],
    engine=embed_model
)

# retrieve from Pinecone
xq = res['data'][0]['embedding']

# get relevant contexts (including the questions)
res = index.query(xq, top_k=5, include_metadata=True)

# get list of retrieved text
contexts = [item['metadata']['text'] for item in res['matches']]

augmented_query = "\n\n---\n\n".join(contexts)+"\n\n-----\n\n"+query
```

Here you can adjust with your own question. In this case, I use PDF of Langchain documentation. It is because Langchain was only released a couple of months ago. So, we ask "What is Langchain?"

### Setting the prompt for GPT

```
# You can change this prompt into anything really.
primer = f"""You are Q&A bot. A highly intelligent system that answers
user questions based on the information provided by the user above
each question. If the information can not be found in the information
provided by the user you truthfully say "I don't know".
"""

res = openai.ChatCompletion.create(
    model="gpt-3.5", #or gpt-4 if you have access to it
    messages=[
        {"role": "system", "content": primer},
        {"role": "user", "content": augmented_query}
    ]
)

response = res['choices'][0]['message']['content']
result = f"Question: {query}\n\nAnswer: \n\n{response}"
```

It is just one example of prompting. You can experiment a whole lot with it.

### Show the result. Voila!

```
from IPython.display import Markdown

display(Markdown(result))
```
### Let's compare the result

When we ask "What is Langchain?", here's the result:

![alt text for screen readers](/images/pdf_result.png "Result New") 

Compare it to the result from ChatGPT with GPT-3.5 and GPT-4
![alt text for screen readers](/images/gpt-result1.png "Result 1") 
![alt text for screen readers](/images/gpt-result2.png "Result 2") 

Pretty cool, huh? And you can do this with any document you want!
